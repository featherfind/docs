\include{KECReportFormat} %includes the file KecReportFormat.tex that include all  necessary formattings
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Define Macros for Details of your Project
\newcommand{\project}{Major Project } %Specify "Major Project" or "Minor Project"
\newcommand{\projectTitle}{FEATHERFIND : BIRD SPECIES IDENTIFICATION FROM AUDIO
    AND IMAGE} %specify "Title" of Your Project
\newcommand{\doc}{Proposal} % specify the document you are preparing eg. "Proposal", "Mid-Term Report" or "Final Report"32
% Note that You have to sibmit "Final Report" for Pre-final defense as well.
\newcommand{\subCode}{CT707} %specify Subject of Your Project
\newcommand{\degree}{Bachelor in Computer Engineering}
%specify your degree
\newcommand{\submittedBy}%Specify Names and Roll/Symbol Numbers of the Project Group Members
{
    %Edit Member Names and Roll/Symbol No. and adjust width (\makebox[width]) if necessary 
    \makebox[9.9cm]{Gaurav Giri \hfill [Kan077bct034]}\\
    \makebox[10cm]{Iza K.C. \hfill [Kan077bct039]}\\
    \makebox[10cm]{Prajwal Khatiwada \hfill [Kan077bct056]}\\
    \makebox[10cm]{Samrat Kumar Adhikari \hfill [Kan077bct074]}\\

    %\makebox[9cm]{Member Name \hfill [Roll/Symbol No.]}\\
} % Note that You must write your "Symbol Numbers"(Exam Roll Numbers) for Final Defenses

\newcommand{\submittedTo}{Department of Computer and Electronics Engineering}
%specify your department
\newcommand{\hod}{Er. Rabindra Khati \\ Associate Professor}
%specify Head ot the department
\newcommand{\defYear}{2024} %Defense Year
\newcommand{\defMonth}{June} %Defense Month- January, February, ...
%\newcommand{\defDay}{5} %specify Defense Day- 1, 2, ...

\newif\ifhassupervisor
\hassupervisorfalse % to display supervisor name use command- \hassupervisortrue

\newcommand{\supervisor}{Er. Bishal Thapa}
\newcommand{\degSup}{Project Coordinator\\ Department of Computer and
    Electronics Engineering}
\newcommand{\external}{External's Name}
\newcommand{\degExternal}{Designation of External\\Second line if required}
%Specify Name of External for Major Project (Required for Black Book) , use multiple lines (\\) if necessary

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Define Abberviations and Symbols
% NOTE that Only those Abberviations and Symbols that are included in document(using command \ac{}) will be displayed in the List of Abberviations and Symbols.

%class 'abbr': for List of Abbreviations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% class `symbol': for List of Symbols
%\DeclareAcronym{transparencyFactor}{
%  short = \ensuremath{\alpha} ,
% long  = Transparency Factor ,
% sort  = Transparency Factor , % string to compare for sorting symbols... default string is the acronym name -"transparencyFactor"
%class = symbol
%}% declares acronym named "transparencyFactor". Use \ac{UN} for short and \acl{UN} for long form.

%\DeclareAcronym{areaOfTriangle}{
% short = \ensuremath{a} , % use \ensuremath{a} instead of $a$
%% long  = Area of Triangle ,
% sort  = Area of Triangle , % string to compare for sorting symbols
%class = symbol
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{secnumdepth}{3}
%The Document
\begin{document}

\KECcoverpage
\KECtitlepage

\pagenumbering{roman}
%\KECapproval
% \chapter*{Acknowledgment}
% \addcontentsline{toc}{chapter}{Acknowledgment}%to include this chapter in TOC
% We would like to express our gratitude to everyone who helped us to complete this project.
% First and foremost, we would like to acknowledge the crucial role of our teachers of Department of Electronics and Computer Engineering for their guidance, support, and feedback throughout the project. Next, we would like
% to give our gratitude to our classmates, for providing constructive feedback and engaging in
% thought-provoking discussions regarding our project. Next, we’d like to thank all lecturers
% of our department for guiding us through the beginning of our project to the end. Finally,
% a special gratitude to our family and friends, for their love, encouragement, and support
% throughout our academic journey.\\
% Thank you all for your invaluable contributions to this project.\\
% \makebox[10cm]{Ajaya Chaudhary \hfill }\\
% \makebox[10cm]{Gaurav Giri \hfill }\\
% \makebox[10cm]{Iza K.C. \hfill }\\
% \makebox[10cm]{Lakesh Shrestha \hfill }
%to display members name under Acknowledgement
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}%to include this chapter in TOC 
This report presents "FeatherFind", a comprehensive model designed for
identifying bird species using both audio recordings and images. The audio
identification process involves collecting and preprocessing bird sound
datasets, isolating key features using methods like Mel-Frequency Cepstral
Coefficients (MFCC), and employing a combination of Convolutional Neural
Networks (CNN) and Long Short-Term Memory networks (LSTM) to accurately
recognize bird calls. For image identification, the model utilizes the VGG16
architecture, a type of deep CNN, to analyze bird photos. This involves
enhancing image quality and extracting features such as color, size, shape, and
patterns, which are then compared to a comprehensive database of bird images to
identify the species. The integrated approach aims to enhance the accuracy and
efficiency of bird species identification, facilitating better conservation
efforts, biodiversity monitoring, ecological research, and environmental
education. By combining advanced machine learning techniques with extensive
datasets, FeatherFind promises to offer a robust solution for the automated identification
and monitoring of bird species, particularly those that are endangered, thereby
contributing significantly to the field of avian research and conservation.
    % {\textit{Keywords$-$Optical Character Recognition, Binarization, Thresholding, Denoising, Boundary Detection, Hough Line Transformation, K-Means Clustering, Convolutional Neural Network}}

    %{\par
    %\begin{flushright}
    %\vskip -20pt
    %\setstretch{1.2}
    %\submittedBy
    %\end{flushright}}

    %to adjust spacings for TOC, LOF, LOT
    {

        %TOC, LOF and LOT
        \KECadjusttocspacings % defined in KECReportFormat.tex to adjust spacings
        \makeatletter
        % to add vskip of 18 point which is reduced when parskip is set to 0 in \LECadjustspacings
        \def\@makeschapterhead#1{%
            %\vspace*{50\p@}% Remove the vertical space
            {\newpage \parindent \z@ \raggedright \normalfont
                    \interlinepenalty\@M
                    \center\fontsize{16pt}{1} \bfseries
                    \MakeUppercase{#1}\par\nobreak
                    \vskip 18\p@ % adjust space after heading 18pt
                }}
        \makeatother

        \tableofcontents % prints table of contents
        \listoffigures % prints list of figures
        \addcontentsline{toc}{chapter}{List Of Figures}
        %\listoftables % prints list of table
    }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%comment this chapter if you don't have List of Abbreviations
%\KECloa % defined in KECReportFormat

%comment this chapter if you don't have List of Symbols
%\KEClos % defined in KECReportFormat

\chapter*{Abbreviations}
\addcontentsline{toc}{chapter}{Abbreviations}
\begin{tabbing}
    \hspace{50mm}\=\kill
    CNN \> Convolutional Neural Network\\
    CUB \> Caltech-UCSD Birds\\
    DCNN \> Deep Convolutional Neural Network\\
    DCT \> Discrete Cosine Transform\\
    DFT \> Discrete Fourier Transform\\
    GA \> Genetic Algorithm\\
    GPS \> Global Positioning System\\
    GRU \> Gated Recurrent Network\\
    LSTM \> Long Short-Term Memory\\
    MAP \> Mean Average Precision\\
    MFCCs \> Mel-Frequency Cepstral Coefficients\\
    MLP \> Multilayer Perceptron\\
    RGB \> Reg Green Blue\\
    ROC \> Receiver Operating characteristics\\
    RNN \> Recurrent Neural Network\\
    STFT \> Short-Time Fourier Transform\\
    SGD \> Stochastic Gradient Descent\\
    UCSD \> University of California San Diego\\
    VGG \> Visual Geometry Group\\
    % \DeclareAcronym{CNN}{
    %  short = \ensuremath{a} , % use \ensuremath{a} instead of $a$
    %  long  = Convolutional Neural Network,
    %  sort  = Convolutional Neural Network, % string to compare for sorting symbols
    %   class = symbol
    % }
    % \DeclareAcronym{EMNIST}{
    %  short = \ensuremath{a} , % use \ensuremath{a} instead of $a$
    %  long  = Extended Modified National Institute of Standards and Technology,
    %  sort  = Extended Modified National Institute of Standards and Technology, % string to compare for sorting symbols
    %   class = symbol
    % }
\end{tabbing}
\newpage
\pagenumbering{arabic}

\chapter{Introduction}
\section{Background}\label{sec:bkgrnd}%label your section if you require to refer them somewhere else in your document.
Globally, the avian kingdom is vast, with over 11,000 species, a testament to
nature's complexity
and evolutionary prowess. This figure, sourced from the International
Ornithological Committee as
of April 2023, merely scratches the surface of avian diversity, each species a
unique entity with
its own ecological role and evolutionary story.\cite{ioc_updates}

Turning our gaze to Nepal, a country of remarkable biodiversity and varied
ecosystems, from the lowland Terai to the towering Himalayas, it is home to
more than 887 bird species, as reported by the Himalayan Nature organization.
This represents more than 8\% of the world's known bird species, a significant
figure given Nepal's relatively small geographical footprint.\cite{himalayan}

Among these, a number are endangered, their existence threatened by habitat
loss, climate change, and human activities. The National Red List of Nepal's
Birds, a comprehensive assessment of the country's avian biodiversity,
identifies several species at risk. Specifically, Nepal harbors 168 nationally
threatened bird species, including 68 Critically Endangered, 38 Endangered, and
62 Vulnerable species, as detailed in a publication by the Journal of
Threatened Taxa.\cite{inskipp2017nepala}

The plight of these endangered species underscores the urgency of conservation
efforts. Technologies such as audio recognition and image classification offer
innovative tools for identifying and monitoring bird populations. By analyzing
the unique sounds and visual characteristics of birds, researchers can enhance
our understanding of species distribution, behavior, and threats. Such
technologies not only aid in the conservation of endangered species but also
contribute to the broader field of biodiversity research.

\section{Problem Statement}
In Nepal, a hotspot of avian biodiversity, accurately identifying and
classifying bird species, particularly those that are endangered, is a critical
yet complex task. Traditional observation methods are limited by the vast
geographical and ecological diversity of the region, making it challenging to
monitor and protect these birds effectively. The necessity for precise
identification is paramount for conservation efforts aimed at maintaining
ecosystem balance. To address this, there is a pressing need for a method that
can overcome these constraints by leveraging advanced technologies capable of
distinguishing between the myriad of bird calls and songs, as well as visual
markers through image classification. Such a method promises to automate the
identification process, enhancing accuracy and efficiency in monitoring
endangered species.

% Implementing audio recognition and image classification technologies, however,
% raises several challenges, including the effective training of these systems to
% recognize the specific calls and visual markers of Nepal's endangered birds.
% This requires collecting and curating extensive datasets, a task complicated by
% the elusive nature of many species and the complex acoustics of their habitats.
% Additionally, integrating these technologies into conservation strategies is
% crucial for not just identifying but also protecting these species from various
% threats. A multidisciplinary approach, combining expertise in ornithology,
% conservation biology, machine learning, and environmental science, is essential
% to develop a robust system. Such a system could provide reliable data on
% species distribution, population trends, and habitat use, thereby informing
% targeted conservation actions and policies to preserve Nepal's rich avian
% biodiversity.

\section{Objectives}
\begin{enumerate}[label=\roman*]
    \item To develop and implement an integrated technological solution that
          utilizes
          advanced audio recognition and image classification techniques for
          the accurate
          identification and monitoring of bird species in Nepal, with an
          emphasis on
          endangered species.
\end{enumerate}

\section{Application Scope}
\begin{enumerate}
    \item \textbf{Conservation Efforts:}\\This system will enhance conservation
          by enabling accurate monitoring
          of bird populations, helping track endangered species and take a step
          towards habitat protection.
    \item \textbf{Biodiversity Monitoring:}\\Automated identification will aid
          biodiversity monitoring by
          processing large datasets, helping detect species distribution on
          bird communities.
    \item \textbf{Ecological Research:}\\ Researchers can use the system to
          study bird migration,
          and habitat use, providing crucial data for modeling ecosystems and
          understanding ecological interactions.
    \item \textbf{Environmental Education and Awareness:}\\Integrated into
          educational programs, this tool will
          raise public awareness about biodiversity and conservation, engaging
          students and scientists in bird identification.
    \item \textbf{Bird viewing:}\\Bird enthusiasts will benefit from this
          system as it will enhance bird watching
          experiences by providing instant identification of bird species
\end{enumerate}
\section{Features}
\begin{enumerate}
    \item \textbf{Species Identification Using Audio:}\\
          The app allows users to record bird sounds in real-time using their
          device's microphone or
          upload pre-recorded audio files. Advanced noise filtering techniques
          isolate bird calls from
          background noise, and sound wave analysis helps in identifying
          distinct frequency patterns.
          Machine learning algorithms, trained on a vast database of bird
          calls, match the recorded
          sound to identify the bird species accurately.

    \item \textbf{Species Identification Using Image:}\\
          Users can capture photos of birds using the app's camera or upload
          images from their gallery.
          The app enhances image quality and analyzes features such as color,
          size, shape, and patterns.
          Utilizing computer vision models, the app identifies the bird species
          by comparing the image
          with a comprehensive database of bird images.

    \item \textbf{Mapping Identified Bird Habitat:}\\
          The app tags the location of identified birds using GPS, providing
          detailed habitat information
          typical of each species. Integrating with mapping services, it
          displays bird sightings on an
          interactive map, generating heat maps to show species density and
          distribution. Additionally,
          it tracks and visualizes bird migration patterns over time, helping
          users understand seasonal
          movements.

    \item \textbf{Provide Description About the Birds:}\\
          For each identified bird species, the app offers detailed profiles
          that include scientific and
          common names, physical descriptions, and conservation status. It also
          provides audio and visual
          media for reference, along with information on the bird's behavior,
          diet, and typical habitats,
          enriching the user's understanding of the species.

\end{enumerate}
\section{Feasibility Study}
Before implementation of project design, the feasibility analysis of the
project must be done to move any further. The feasibility analysis of the
project gives an idea on how the project will perform and its impact in the
real world scenario. So, it is of utmost importance.
\subsection{Economic Feasibility}
Our system is economically achievable as a result of the development of several
tools, libraries, and frameworks. Since all the software required to construct
it is free and readily available online, this project is incredibly
cost-effective. Only time and effort are needed to create a worthwhile,
genuinely passive system. The project doesn't come at a substantial cost. From
an economic standpoint, the project appears successful in this sense.

\subsection{Technical Feasibility}
The software needed to implement a project can be downloaded from a wide
variety of online resources. Technically speaking, the project is feasible as
the necessary software is easily available. We were able to learn the
information we required for the project through a variety of online sources,
including classes. All the libraries and data are accessible online for free
because this project does not require any licensing costs. It is technically
possible if one has the necessary information and resources.

\subsection{Operational Feasibility}
The project aims to enhance bird species identification through audio
classification, making bird sound recognition more accessible and efficient.
This solution is particularly beneficial for ornithologists, bird watchers, and
environmental researchers who require accurate and quick identification of bird
species based on their calls. The project leverages advanced audio signal
processing and deep learning algorithms to classify bird sounds, ensuring high
accuracy and reliability. Given the widespread availability of mobile devices
and recording equipment, the project is operationally feasible, as it can be
easily integrated into existing workflows and tools used by bird enthusiasts
and professionals. By providing an efficient method for bird sound
classification, the project supports a sizable community interested in avian
studies and conservation, ensuring practical applicability and ease of use.

\subsection{Schedule Feasibility}
The workload of the project is divided amongst the project members. The
scheduling is done according to an incremental model where different modules
are planned to be assigned to the group members.So, the project fulfills the
schedule feasibility requirements.
%(GAntt chart here)
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{images/GanttChart.png}
    \caption{Gantt Chart}\label{fig:my_label}
\end{figure}
\newpage
\section{System Requirements}

\subsection{Development Requirements}
\begin{table}[h]
    \centering
    \caption{Development Requirements}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Software Requirements}                 & \textbf{Hardware
        Requirements}                                                          \\ \hline
        Programming Language: Python, Dart, Javascript & Camera: \(>\)= 12
        Megapixels                                                             \\ \hline
        Design Tools: Figma, Canva, Draw.io            & RAM: \(>\)= 8 GB
        \\ \hline
        Libraries: Librosa, PyAudio, Pytorch           & CPU: i5 10th
        (Recommended)                                                          \\ \hline
        Framework: Flutter, Django RestFramework       & GPU: P100
        \\ \hline
        IDEs: VSCode, Android Studio                   & Storage: \(>\)= 50 GB
        \\ \hline
    \end{tabular}
\end{table}

\subsection{Deployment Requirements}
\begin{table}[ht]
    \centering
    \caption{Deployment Requirements}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Software Requirements} & \textbf{Hardware Requirements}
        \\ \hline
        Android: \(>\)= 10             & Camera: \(>\)= 12 Megapixels
        \\ \hline
        Read/Write FileSystem          & RAM: \(>\) 4 GB
        \\ \hline
        Internet Accessibility         & Storage: \(>\)= 20 GB
        \\ \hline
        Database: Sqlfite              & Recording Quality \(>\)= 256 Kbps, 48
        KHz                                                                    \\ \hline
    \end{tabular}
\end{table}

\chapter{Literature Review}

This literature review explores the progression of methodologies and
technologies in the field,
with a particular focus on the use of Convolutional Neural Networks (CNN) and
Long Short-Term Memory (LSTM)
networks for audio-based bird species identification. The review also examines
the challenges associated
with dataset quality and diversity, and the innovative strategies employed to
address these issues,
providing a comprehensive overview of the current state of research and future
directions in avian bioacoustics.

\section{Related Works}
BirdNET is a cutting-edge research platform developed through collaboration
between the K. Lisa Yang Center for Conservation Bioacoustics at the Cornell
Lab of Ornithology and the Chair of Media Informatics at Chemnitz University of
Technology. Its primary aim is to detect and classify bird sounds using machine
learning technologies, serving both experts and citizen scientists in their
efforts to monitor and protect bird populations.\\\\ BirdNET can identify
around 3,000 of the world's most common bird species, with plans to expand this
number. Features such as a live submissions map and a Twitter bot are included
to engage the community and share real-time data. The project is supported by
donations and collaborations, offering opportunities for researchers and
developers to contribute to its growth. BirdNET serves as an invaluable tool
for bird enthusiasts, conservationists, and biologists alike, providing
innovative solutions for large-scale acoustic monitoring and contributing to
the conservation and understanding of avian biodiversity.\\\\ The BirdCLEF 2023
competition on Kaggle is a significant data science challenge that falls under
the broader LifeCLEF initiative, aimed at pushing the boundaries of species
identification and biodiversity monitoring through technological innovation.
This particular competition focuses on the development of machine learning
models that can identify bird species based on audio recordings. It presents a
complex and realistic challenge due to the diversity of the audio recordings,
which are collected from various environments and feature a wide range of bird
species.
\section{Related Research}

The research paper `Audio Classifier for Automatic Identification of Endangered
Bird Species of Nepal' focuses on developing an audio classifier to identify
endangered bird species in Nepal using deep learning techniques. The dataset,
collected from xeno-canto.org, comprises 2215 audio recordings of 41 bird
species, 38 of which are endangered. This dataset was expanded to 6733
recordings through 10-second audio splitting and Gaussian noise augmentation,
with 5407 recordings used for training, 639 for validation, and 687 for
testing. The methodology involved handling imbalanced class distribution
through data augmentation, employing Mel spectrograms and Mel-Frequency
Cepstral Coefficients (MFCCs) for feature extraction, and developing a custom
Convolutional Neural Network (CNN) model and an EfficientNet model. The
hyperparameters of these models were optimized using a genetic algorithm. The
Mel spectrograms were created using Short-Time Fourier Transform, converting
amplitudes to decibel scale, and applying Mel filter banks to the spectrograms.
Similarly, MFCCs were derived by framing the audio signals, applying Discrete
Fourier Transform, logarithmic scaling, Mel scaling, and Discrete Cosine
Transform. The EfficientNet architecture utilized compound scaling for network
depth, width, and resolution. The findings indicated that the proposed approach
achieved satisfactory results in classifying the bird species. However,
limitations include the relatively small dataset size and the need for further
enhancement in model robustness and accuracy.\cite{gautam2023audio}
%Future enhancements could involve
% expanding the dataset, exploring additional feature extraction techniques, and
% incorporating more advanced deep learning models to improve classification
% performance.\cite{gautam2023audio}\\

The paper `Audio Bird Classification with Inception-v4 extended with Time and
Time-Frequency Attention Mechanisms' presents an innovative adaptation of the
Inception-v4 deep convolutional network for bioacoustic classification,
focusing specifically on bird sound recognition. The datasets employed include
various bird sounds, prominently from the BirdClef2017 challenge, consisting of
1500 bird species recordings. The methodology revolves around treating bird
sound classification as an image classification problem through transfer
learning. The Inception-v4 model, initially pre-trained on ImageNet, was
adapted to process time-frequency representations of bird sounds by converting
these sounds into RGB images using three log-spectrograms generated via fast
Fourier transform at different scales (128, 512, 2048 bins).The findings
demonstrate that the model,
termed `Soundception', integrates time and time-frequency attention mechanisms
effectively, significantly improving classification accuracy. The results
highlight Soundception's outstanding performance, achieving a mean average
precision (MAP) of 0.714 in classifying 1500 bird species, 0.616 MAP for
background species, and 0.288 MAP for soundscapes with time-codes, making it
the top model in the BirdClef2017 challenge across multiple tasks. However,
limitations include the incomplete convergence of the model due to
computational constraints and the extensive GPU resources required for
training. The paper concludes with a discussion on future
improvements, such as exploring different scalable optimizations and
incorporating stacked GRU layers for better audio-to-image representation
learning, underscoring the potential of transfer learning from advanced image
classification models to acoustic domains.\cite{sevilla2017audio}\\

The research paper `Bird Species Identification using Deep Learning' presents a
methodology using Deep Convolutional Neural Networks (DCNNs) to classify bird
species, leveraging the Caltech-UCSD Birds 200 (CUB-200-2011) dataset, which
contains 11,788 annotated images of 200 bird species. The methodology involves
converting images to grey scale to reduce computational complexity, followed by
the application of DCNNs using TensorFlow to extract hierarchical features from
images such as edges, textures, and complex patterns. The neural network
architecture includes convolutional layers for feature extraction, pooling
layers for dimensionality reduction, activation layers for non-linearity, and
fully connected layers for classification. Key findings show the DCNN model
achieved a testing accuracy of 80\% and training accuracy of 93\%, with a
validation accuracy of around 75\%, indicating robust performance across
different data splits. The study highlights the effectiveness of combining
multiple features (head, body, color, beak) over single-feature classification,
with generated autographs and score sheets facilitating identification.However,
the research also identifies limitations such as the dependency on the quality
and diversity of the dataset,potential overfitting due to the complexity of the
model, and the computational resources required for training deep learning
models\cite{Gavali2019Bird}.\\

The research paper `Bird species classification from an Image using VGG-16
Network' utilizes machine learning which has shown
significant advancements in both methodology and accuracy. The primary focus
has been on the creation and utilization of diverse datasets, with a notable
example being the dataset of 1600 images across 27 bird species, as well as the
Caltech-UCSD Birds-200-2011 Dataset. Methodologically, the use of Convolutional
Neural Networks (CNNs), particularly the VGG-16 model, has been prevalent due
to its effectiveness in feature extraction from images. This approach has been
complemented by transfer learning techniques, which have notably improved
classification accuracies, as seen in the use of pre-trained networks like
AlexNet and VGG-16, achieving accuracies up to 85.4\% and 92.13\% respectively.
The application of machine learning algorithms such as SVM with a linear kernel
and KNN has also been explored, with SVM achieving an accuracy of 89\% after
parameter optimization. Additionally, the integration of computer vision
methods to extract Histogram Oriented Gradients and RGB histogram values has
further enhanced classification performance. Despite these advancements, the
studies have encountered limitations, including the challenge of accurately
classifying bird species from various angles and positions, and the need for
extensive preprocessing to remove noise from datasets. Overall, the body of
work demonstrates a trend towards higher accuracy in bird species
classification through the innovative use of deep learning techniques and
sophisticated feature extraction methods, though challenges remain in dealing
with the variability of natural images and the computational demands of
processing large datasets.\cite{islam2019bird}\\

This research paper presents significant advancements in the field of
`Automatic bird species identification through the integration of audio signal
processing and neural networks'. The study, conducted by Chandu B et al.
outlines a robust methodology for identifying bird
species from audio recordings, leveraging a combination of meticulously curated
datasets and  machine learning techniques. The dataset was manually compiled
from both local recordings and
online resources such as xeno-canto.org, which apart from bird songs also
contains ambient noise and
human voices to simulate real-world conditions. Pre-processing techniques
including pre-emphasis, framing, silence removal, and reconstruction were
applied to the audio clips to enhance the relevant frequency components and
eliminate unnecessary noise, ensuring the purity of the dataset. Spectrograms
of these processed clips were generated and used as input for training a
convolutional neural network (CNN), specifically AlexNet, chosen for its high
accuracy in image classification tasks. Through transfer learning, AlexNet was
adapted to recognize bird species from the spectrograms, achieving a
classification accuracy of 97\% in controlled environments. However,
recognizing the variability of real-world conditions, the researchers retrained
the model with datasets containing ambient noise, achieving a real-time
classification accuracy of 91\%. Despite these promising results, the study
acknowledges limitations such as the relatively small size of the dataset and
the need for further tuning of performance parameters to improve
robustness\cite{chandu2020automated}.\\

The paper `An Ensemble of Convolutional Neural Networks for Audio
Classification' delves into a comprehensive study on CNN classification using
different architectures, data augmentation techniques, and audio signal
representations, aimed at enhancing audio classification tasks across various
datasets. The study employs three datasets: BIRDZ, CAT, and ESC-50, each
offering unique challenges in audio classification. The methodology involves
training five convolutional neural networks (CNNs) with four audio
representations combined with six different data augmentation methods,
resulting in thirty-five subtypes of ensembles. The audio representations
include techniques such as the Discrete Gabor Transform (DGT), Waveform
Similarity OverLap Add (WSOLA), and Phase Vocoder. The data augmentation
methods encompass procedures like short spectrogram augmentation, random time
shift, and frequency masking. The CNN architectures are pre-trained models
fine-tuned with these augmented datasets to boost classification accuracy. The
findings reveal that the ensemble method outperforms standalone networks,
achieving 97\% accuracy on the BIRDZ dataset, 90.51\% on the CAT dataset, and
88.65\% on the ESC-50 dataset. The study also highlights that the
best-performing CNNs are VGG16 and VGG19, with DGT as the most effective signal
representation. However, the study acknowledges limitations, such as the
computational cost of training ensembles and the variability in performance
across different augmentation techniques\cite{nanni2021ensemble}.\\

The literature review focused on the `analysis of bird call datasets sourced
from Xeno-Canto', comprising 72,172 samples from 264 bird species in 16-bit wav
format with a 16 kHz sampling rate. The methodology involved preprocessing the
audio data to filter out low-frequency noise and normalize signal amplitude,
followed by generating Mel-spectrograms and Mel-Frequency Cepstral Coefficients
(MFCCs) as inputs for deep learning models. The Mel-spectrograms were produced
using discrete Fourier transform (DFT), and the MFCCs were derived by applying
discrete cosine transform (DCT) to the Mel-spectrogram. The study employed
various metrics to evaluate the performance of these methods, including ROC
analysis to visualize model effectiveness. Findings indicated that the proposed
models showed significant promise in identifying bird species from their calls,
with improvements in classification accuracy compared to previous approaches.
However, limitations were noted, including potential biases in the dataset due
to uneven sample distribution across species and the challenge of background
noise affecting signal quality. Future work suggested enhancing noise reduction
techniques and exploring more sophisticated neural network architectures to
further improve model robustness and accuracy.\cite{wang2022efficient}\\

The study conducted an in-depth analysis of `bird species recognition through
acoustic monitoring', utilizing a robust dataset of bird sound samples,
meticulously annotated and validated for accuracy. The dataset, referred to as
SD, comprises multispecies bird sound recordings, each labeled with species
name and sample ID, along with corresponding metadata, providing a
comprehensive foundation for model training and evaluation. Methodologically,
the research employed a spectrogram-based feature extraction approach,
leveraging Short-Time Fourier Transform (STFT) to capture the intricate
temporal and spectral characteristics of bird sounds. This was followed by the
application of a Multilayer Perceptron (MLP) classifier to distinguish between
different bird species. The findings reveal that the proposed model achieved
high recognition accuracy, with some species being identified with perfect
precision, recall, and accuracy (100\%), though the performance varied across
species, with a few showing lower recognition rates (86.9\%) and
precision/recall values ranging between 50-75\%. The results demonstrated an
overall classification accuracy of 96\%, with cross-validation accuracy
standing at 81.4\%, highlighting the model's robustness yet indicating room for
improvement in generalizability across diverse datasets. Despite the promising
results, the study acknowledges several limitations, including the variability
in recognition accuracy among different species and the potential influence of
environmental noise on model performance. Future work is suggested to explore
feature and model fusion techniques, integrate the model with cloud-based
systems for real-time recognition, and expand the dataset to include a broader
range of bird species to enhance the model's applicability and accuracy in
practical scenarios.\cite{pahuja2021sound}\\ \\
\chapter{Methodology}
% \section*{}
This chapters describes bird identification using audio and image separately.
\section{Working Mechanism for Identification using Audio}
% https://www.geeksforgeeks.org/vgg-16-cnn-model/
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.33]{images/Methodology1.png}
    \caption{Block diagram for the working mechanism of the
        system}%\label{fig:fig1}
\end{figure}
\newpage
The working mechanism for bird identification from audio utilizes the CharaNET
dataset,
housing a diverse collection of avian vocalizations. Our methodology revolves
around the
utilization of Convolutional Neural Networks (CNN) and Long Short-Term Memory
(LSTM) architectures.
The primary objective of this study is to achieve high levels of accuracy in
identifying a
broad spectrum of 41 distinct bird species. Here lies the detailed explaination
of the methodology
from working mechanism to model training.
\subsection{Dataset}
For this project, we will utilize the CharaNet dataset, which is specifically
curated to include
audio files of Nepal's endangered bird species. This dataset was collected from
\textit{xeno-canto.org},
a platform where bird sounds from around the world are shared by contributors
who travel extensively to
capture these sounds. From this source, we will gather 2215 audio recordings
representing 41 bird species,
38 of which are listed as endangered in Nepal.

To augment the dataset, we propose employing a 10-second split method, which
will expand the dataset to 6733
audio recordings. Additionally, for bird species with fewer than 30 recordings,
we will add Gaussian noise to
further increase the number of samples. This augmentation process is expected
to result in a robust dataset
comprising 5407 audio recordings for training, 639 for validation, and 687 for
testing. This comprehensive
dataset will ensure that our model can learn effectively and generalize well to
new, unseen data.

This augmentation methodology and the CharaNet dataset have been referenced in
prior research\cite{gautam2023audio},
demonstrating the efficacy of such an approach. \newpage
\subsection{Dataset Overview for Audio}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/TrainingDataset.png}
    \caption{Training Dataset for audio}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/ValidationDataset.png}
    \caption{Validation Dataset for audio}
\end{figure}
\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/TestDataset.png}
    \caption{Testing Dataset for audio}
\end{figure}
\hspace{3.5cm}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.3]{images/SampleData.png}
    \caption{Sample of the Data}
    \label{fig:SampleData}
\end{figure}
\newpage
\subsection{Data Preprocessing}

The collected audio data undergoes thorough preprocessing to ensure its
suitability for model training. This process includes:
\begin{itemize}
    \item \textbf{Silence Gap Removal:} After observing some of the data
          samples, it was found that the audio has some silence gaps
          in either of the ends as seen in \ref{fig:SampleData}. These could be
          removed employing the silence removal algorithm. The processing
          starts from both ends and moves toward the center. In that processing,
          the local mean of the window segment of the audio wave is
          calculated and compared with the audio’s global mean. If the local mean
          is smaller than the global mean, the window segment is c
          onsidered to contain insignificant data, thus the segment is clipped
          from the original audio\cite{9850832}. Algorithm \ref{alg:Silence_Removal}
          clarifies this.
    \item \textbf{Segmentation:} Dividing the continuous audio recordings into
          smaller, more manageable segments, ensuring consistency in input length.
    \item \textbf{Cleaning:} Removing any background noise and irrelevant
          sounds that could interfere with the training process.
    \item \textbf{Labeling:} Assigning the correct bird species labels to each
          audio segment, which is crucial for supervised learning.
\end{itemize}

\subsection{Data Splitting}

After preprocessing, the data is split into three sets:
\begin{itemize}
    \item \textbf{Training Data:} Used to train the machine learning models.
    \item \textbf{Testing Data:} Used to evaluate the model's performance
          during training.
    \item \textbf{Validation Data:} Used to validate the final model's
          performance and prevent overfitting.
\end{itemize}

This structured approach ensures that the model can learn effectively from the
training data while being evaluated on unseen data to measure its
generalization capabilities.

\subsection{Data Augmentation}

To further enhance the robustness of the model, data augmentation techniques
are applied to the training data. This includes adding various types of noise
to the audio segments to create more diverse training samples and prevent the
model from overfitting to specific patterns in the data.
\begin{eqnarray}
    \text{Augmented Data} = \text{Original Data} + \text{Noise}
\end{eqnarray}

By introducing these variations, the model becomes more resilient to different
audio conditions and better at generalizing to new recordings.

\subsection{Audio Splitting}

The augmented audio data is then split into smaller 3-second clips. This
standardization ensures that the input size remains consistent, which is
essential for the feature extraction and modeling stages.

\subsection{Feature Extraction}

Feature extraction is a critical step where the audio clips are transformed
into a format that can be fed into machine learning models. We use two primary
techniques for feature extraction: Spectrograms and Mel-Frequency Cepstral
Coefficients (MFCC).

\subsubsection{Spectrogram}

A spectrogram is a visual representation of the spectrum of frequencies in a
sound signal as they vary with time. It is generated by applying the Short-Time
Fourier Transform (STFT) to the audio signal. This transformation provides
insight into how the frequency content of the signal changes over time.
\begin{eqnarray}
    X(n) = \sum_{m=0}^{N-1} x(m) \cdot w(n-m)
\end{eqnarray}
% \[
% X(n) = \sum_{m=0}^{N-1} x(m) \cdot w(n-m)
% \]

where \( x(m) \) is the audio signal and \( w(n) \) is the window function.

\subsubsection{Mel-Frequency Cepstral Coefficients (MFCC)}

MFCCs are coefficients that collectively describe the short-term power spectrum
of a sound signal. The process of obtaining MFCCs involves several steps:
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{images/MFCC.jpg}
    \caption{Feature Extraction Using Spectrogram and
        MFCC}%\label{fig:fig1}
\end{figure}
\begin{enumerate}
    \item \textbf{Framing:} Divide the audio signal into short frames.
    \item \textbf{Discrete Fourier Transform (DFT):} Convert each frame to the
          frequency domain.
          \begin{eqnarray}
              X(k) = \sum_{n=0}^{N-1} x(n) \cdot e^{-j \frac{2\pi}{N} kn}
          \end{eqnarray}

    \item \textbf{Log Function:} Apply a logarithm to the amplitude spectrum.
          \begin{eqnarray}
              S_{\text{log}}(k) = \log(|X(k)|)
          \end{eqnarray}

    \item \textbf{Mel-Scaling:} Map the frequencies to the Mel scale, which
          better represents how humans perceive sound.
          \begin{eqnarray}
              f_{\text{mel}} = 2595 \cdot \log_{10}(1 + \frac{f}{700})
          \end{eqnarray}

    \item \textbf{Discrete Cosine Transform (DCT):} Convert the Mel spectrum to
          the cepstral domain, yielding the MFCC features.
          \begin{eqnarray}
              C(n) = \sum_{k=0}^{K-1} S_{\text{mel}}(k) \cdot \cos \left(
              \frac{\pi n (k+0.5)}{K} \right)
          \end{eqnarray}

\end{enumerate}

\subsection{Convolutional Neural Networks (CNN)}

Convolutional Neural Networks (CNNs) are highly effective for extracting
spatial features from spectrograms, making them well-suited for audio
classification tasks. CNNs use convolutional layers to detect patterns and
features in the input data by applying convolutional filters across the input
spectrogram.

\begin{figure}[h!]
    \centering
    \includegraphics{images/Convolution.png}
    \caption{CNN architecture}%\label{fig:fig1}
    % citation link: https://medium.com/techiepedia/binary-image-classifier-cnn-using-tensorflow-a3f5d6746697
\end{figure}

The basic operations involved in a CNN include:
\begin{itemize}
    \item \textbf{Convolution:} This operation involves applying a set of
          learnable filters (kernels) across the input spectrogram to produce feature
          maps.
          \begin{eqnarray}
              (f * x)(t) = \sum_{\tau = -\infty}^{\infty} x(\tau) \cdot f(t -
              \tau)
          \end{eqnarray}

    \item \textbf{Activation Function:} The rectified linear unit (ReLU)
          activation function is applied to introduce non-linearity into the model.
          \begin{eqnarray}
              f(x) = \max(0, x)
          \end{eqnarray}

    \item \textbf{Pooling:} Pooling layers reduce the spatial dimensions of the
          feature maps, typically using
          max pooling to retain the most significant features.
    \item \textbf{Fully Connected Layers:} After several convolutional and
          pooling layers, the feature maps
          are flattened and passed through fully connected layers to produce the
          final classification output.
\end{itemize}

\subsection{Long Short-Term Memory Networks (LSTM)}

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network
(RNN) capable of capturing temporal dependencies in sequential data. This makes
them well-suited for processing audio signals where the order of data points
matters. LSTMs are designed to overcome the limitations of traditional RNNs by
addressing the vanishing gradient problem through the use of gates that
regulate the flow of information.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/LSTM.png}
    % citation link: https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c
    \caption{LSTM architecture}%\label{fig:fig1}
\end{figure}

The key components of an LSTM cell include:
\begin{itemize}
    \item \textbf{Forget Gate:} Determines which information from the previous
          cell state should be discarded.
          \begin{eqnarray}
              f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
          \end{eqnarray}

    \item \textbf{Input Gate:} Decides which new information should be added to
          the cell state.
          \begin{eqnarray}
              i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
          \end{eqnarray}

    \item \textbf{Candidate Cell State:} Creates new candidate values that
          could be added to the cell state.
          \begin{eqnarray}
              \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
          \end{eqnarray}

    \item \textbf{Cell State:} The cell state is updated based on the input
          gate and the forget gate.
          \begin{eqnarray}
              C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
          \end{eqnarray}

    \item \textbf{Output Gate:} Determines the output of the LSTM cell.
          \begin{eqnarray}
              o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
          \end{eqnarray}
          \begin{eqnarray}
              h_t = o_t \cdot \tanh(C_t)
          \end{eqnarray}

\end{itemize}

\subsection{Combined CNN and LSTM (CNN+LSTM)}
A hybrid model that leverages the strengths of both CNN and LSTM architectures
is developed. In this combined model, the CNN processes spectrograms to extract
spatial features, which are then fed into LSTM layers to capture temporal
patterns. The integration of these two models aims to utilize both spatial and
temporal information, thereby improving the overall classification accuracy.
Fully connected layers at the end perform the final classification based on the
features extracted by both CNN and LSTM components.

\subsection{Hyperparameter Optimization using Genetic Algorithm}
Genetic algorithms (GAs) are a powerful method for optimizing hyperparameters
in machine learning models. Genetic Algorithm have proven to significantly
improve the performance metrics of the CNN model instead of using hand tuned
approach for hyperparameters. This section outlines the steps involved in using
GAs for hyperparameter optimization\cite{9058307}.
\begin{itemize}
    \item \textbf{Encoding the Hyperparameters}
          \begin{itemize}
              \item Hyperparameters are represented as a chromosome, where each
                    hyperparameter is a
                    gene in the chromosome.
              \item For example, in a neural network, a chromosome might
                    include genes for the
                    learning rate, number of layers, number of neurons per
                    layer, and activation
                    functions.
          \end{itemize}

    \item \textbf{Initial Population}
          \begin{itemize}
              \item An initial population of chromosomes is generated randomly,
                    with each
                    chromosome representing a different set of hyperparameters.
          \end{itemize}

    \item \textbf{Fitness Function}
          \begin{itemize}
              \item A fitness function is defined to evaluate the performance
                    of each set of
                    hyperparameters.
              \item This typically involves training the model with the given
                    hyperparameters and
                    measuring its performance on a validation set.
          \end{itemize}

    \item \textbf{Selection}
          \begin{itemize}
              \item Selection involves choosing the best-performing chromosomes
                    to serve as parents
                    for the next generation.
              \item Various selection methods can be employed, such as
                    tournament selection,
                    roulette wheel selection, or rank-based selection.
          \end{itemize}
          \newpage
    \item \textbf{Crossover (Recombination)}
          \begin{itemize}
              \item Crossover combines pairs of parent chromosomes to produce
                    offspring for the
                    next generation.
              \item This is done by swapping segments of parent chromosomes to
                    create new
                    chromosomes, thereby combining features of both parents.
          \end{itemize}

    \item \textbf{Mutation}
          \begin{itemize}
              \item Mutation introduces random changes to some of the genes in
                    the offspring
                    chromosomes.
              \item This helps maintain genetic diversity in the population and
                    allows the
                    algorithm to explore a broader search space.
          \end{itemize}

    \item \textbf{Replacement}
          \begin{itemize}
              \item The current population is partially or entirely replaced
                    with the new
                    generation of chromosomes, ensuring that better solutions
                    are carried forward
                    while allowing for exploration of new possibilities.
          \end{itemize}

    \item \textbf{Termination}
          \begin{itemize}
              \item The process of selection, crossover, mutation, and
                    replacement is repeated
                    until a termination criterion is met.
              \item This could be a set number of generations, convergence of
                    fitness scores, or
                    achieving a satisfactory performance level.
          \end{itemize}

    \item \textbf{Best Solution}
          \begin{itemize}
              \item The best chromosome at the end of the process represents
                    the optimal or
                    near-optimal set of hyperparameters for the model.
          \end{itemize}
\end{itemize}
\newpage

\subsection{Algorithms Used}
\subsubsection{Silent Gaps Removal}
\label{alg:Silence_Removal}
\begin{algorithm}
    \caption{Clipping of silent gaps from both ends}
    \begin{algorithmic}[1]
        \STATE $ \text{wav} \gets \text{sampled audio signal} $
        \STATE $ \Delta \gets \text{appropriate window length} $
        \STATE \text{/* In our code, $ \Delta = 500 $ for 16KHz sampling rate
            */}
        \STATE \textbf{INPUT:} $ \text{wav}, \Delta $
        \STATE \textbf{PROCESS:}
        \STATE $ \text{wavAvg} \gets \text{Average}(|\text{wav}|) $
        \STATE $ N \gets \text{Length}(\text{wav}) $

        \STATE \text{/* Removing the silent gap from the start */}
        \FOR{$ \text{idx} = 0, \Delta, 2\Delta, \ldots, N - \Delta $}
        \STATE $ \text{win} \gets \text{wav}[\text{idx} : \text{idx} + \Delta]
        $
        \STATE $ \text{winAvg} \gets \text{Average}(|\text{win}|) $
        \IF{$ \text{winAvg} > \text{wavAvg} $}
        \STATE $ \text{wav} \gets \text{wav}[\text{idx} :] $
        \STATE \textbf{break}
        \ENDIF
        \ENDFOR

        \STATE \text{/* Removing the silent gap from the end */}
        \FOR{$ \text{idx} = N - \Delta, N - 2\Delta, \ldots, 0 $}
        \STATE $ \text{win} \gets \text{wav}[\text{idx} : \text{idx} + \Delta]
        $
        \STATE $ \text{winAvg} \gets \text{Average}(|\text{win}|) $
        \IF{$ \text{winAvg} > \text{wavAvg} $}
        \STATE $ \text{wav} \gets \text{wav}[: \text{idx}] $
        \STATE \textbf{break}
        \ENDIF
        \ENDFOR

        \STATE \textbf{OUTPUT:} $ \text{processed\_wav} \gets \text{wav} $
    \end{algorithmic}
\end{algorithm}

\subsubsection{Genetic Algorithm}
\begin{algorithm}[H]
    \caption{Genetic Algorithm for Hyperparameter Optimization}
    \begin{algorithmic}[1]
        \STATE Initialize the population with random hyperparameters.
        \FOR{generation = 1 to $N$}
        \STATE Evaluate the fitness of each individual in the population.
        \STATE Select individuals to be parents based on their fitness scores.
        \STATE Generate offspring through crossover.
        \STATE Apply mutation to the offspring.
        \STATE Replace the old population with the new generation.
        \ENDFOR
        \STATE Return the best solution found.
    \end{algorithmic}
\end{algorithm}

\subsubsection{Fitness Function}
The fitness function evaluates the performance of hyperparameters by training
and validating the model:

\begin{algorithm}[H]
    \caption{Fitness Function}
    \begin{algorithmic}[1]
        \STATE Train the model with given hyperparameters.
        \STATE Evaluate the model's performance on a validation set.
        \RETURN Performance metric (e.g., accuracy, F1 score).
    \end{algorithmic}
\end{algorithm}

\subsubsection{Mel Spectrogram}
% The mel spectrogram of audio signals are extracted following the given series of steps:
\begin{algorithm}[H]
    \caption{Mel Spectrogram Extraction}
    \begin{algorithmic}[1]

        \STATE \textbf{Input:} Audio signal $x(t)$.
        \STATE \textbf{Output:} Mel spectrogram $S_{\text{mel}}$.
        \STATE Apply STFT to generate spectrogram $S(f, t)$.
        \STATE Convert amplitudes of $S(f, t)$ to dB scale, obtaining
        $S_{\text{dB}}(f, t)$.
        \STATE \textbf{Convert frequencies to Mel scale}.
        \STATE \hspace{10} Choose number of mel bands $N_{\text{mel}}$.
        \STATE \textbf{Construct mel filter banks}.
        \STATE \hspace{10} Convert $f_{\text{min}}$ and $f_{\text{max}}$ of
        $S_{\text{dB}}(f, t)$ to Mel scale.
        \STATE \hspace{10} Divide Mel scale range into $N_{\text{mel}}$
        intervals.
        \STATE \hspace{10} Convert center frequencies of Mel bands back to
        Hertz.
        \STATE \hspace{10} Round center frequencies to nearest bins.
        \STATE \hspace{10} Design triangular band pass filters for each Mel
        band.
        \STATE \hspace{10} Apply mel filter banks to $S_{\text{dB}}(f, t)$ to
        obtain $S_{\text{mel}}$.
    \end{algorithmic}
\end{algorithm}

\subsubsection{Mel-Frequency Cepstral Coefficients Extraction}
% The extraction of MFCCs of a signal involves the following steps:

\begin{algorithm}[H]
    \caption{MFCC Extraction}
    \begin{algorithmic}[1]

        \STATE \textbf{Input:} Audio signal $x(t)$
        \STATE \textbf{Output:} MFCC coefficients.
        \STATE Frame the signal into short frames.
        \STATE Apply a window function to the frames.
        \STATE Apply DFT to generate the frequency spectrum of each frame.
        \STATE Apply logarithm to the spectrum to get log amplitude spectrum.
        \STATE Perform Mel scaling using filter banks to get Mel spectrogram.
        \STATE Apply DCT to the Mel spectrogram to get MFCCs.
    \end{algorithmic}
\end{algorithm}

\newpage

\section{Working Mechanism for Identification using Image}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.33]{images/Methodology2.png}
    \caption{Block diagram for the working mechanism of the
        system}%\label{fig:fig1}
\end{figure}
The working mechanism for the bird identification using image is to be done by
VGG16. VGG16 have been proven
to classify bird speices with accuracy of 92.13\% using Caltech-UCSD
Birds-200-2011 (CUB-200-2011) dataset
which had 1600 images across 27 bird species\cite{islam2019bird}. Our project
aims to achieve similar result for 200 bird species.

\subsection{Dataset}
For the identification of bird species using image,CUB-200-2011
dataset will be, which is an extended version of the CUB-200 dataset. It
consists of
11,788 number of images across 200 species categories\cite{WahCUB_200_2011}.

\subsection{Dataset Overview for Image}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.35]{images/cubdataset.png}
    \caption{Image Dataset distribution for Top 50 Bird Species among 200 Species}
\end{figure}
After observing the distribution of the images across the species, most of them
have 60 images per species while few of them ranges from 45 to 60.

\subsection{Data Preprocessing}
In the context of bird species image classification using deep learning models
like VGG-16, the data preprocessing stage plays a pivotal role in optimizing
model performance and ensuring the input data aligns with the model's
requirements. Here are the key points that highlight the importance of each
preprocessing step:

\begin{enumerate}
    \item \textbf{Image Resizing}: All images are resized to 224x224 pixels
          with 3 RGB channels. This standardization
          is crucial for the model to process images efficiently, as it ensures that
          all input data has a uniform dimensionality,
          matching the model's input size requirement.

    \item \textbf{Normalization}: Pixel values are scaled to a range of 0 to 1.
          Normalization is a critical step because
          it reduces the variability in the input data, which can significantly speed
          up the convergence of the model during training.
          By ensuring that the pixel values are within a similar range, the model
          can navigate the optimization landscape more smoothly,
          leading to better learning outcomes.

    \item \textbf{Data Augmentation}: To enhance the dataset and introduce more
          variability, data augmentation techniques such as flipping,
          rescaling, and shearing are employed. These methods are instrumental in
          creating a more robust dataset by simulating different viewing
          conditions and perspectives. Data augmentation is particularly effective
          in preventing overfitting, as it allows the model to learn
          from a more diverse set of examples.

    \item \textbf{Train-Validation-Test Split}: The dataset is divided into
          training, validation, and test sets. This division
          is essential for a comprehensive training process, where the model is
          trained on a diverse set of images, validated on a
          separate set to adjust hyperparameters without bias, and finally, tested on
          unseen data to assess its generalization capability.
          Such a split ensures that the model can learn effectively and perform
          reliably on new, unseen images.
\end{enumerate}

These preprocessing steps form a comprehensive pipeline that prepares the
dataset for effective training and evaluation, ultimately enhancing the model's
ability to classify bird species accurately.

\subsection{VGG16}
For bird species image classification, the VGG-16 model is a powerful
convolutional neural network (CNN) developed by the Visual Geometry Group at
the University of Oxford. This model includes 13 convolutional layers followed
by 3 fully connected layers, making up its 16 layers. VGG-16's straightforward
yet effective design excels in image classification and object detection. Its
deep stack of convolutional and max-pooling layers captures complex,
hierarchical visual features, allowing the model to distinguish bird species
with high accuracy by recognizing subtle differences in plumage, size, and
shape. Despite newer models, VGG-16 remains relevant and effective for
classifying bird species, making it a preferred choice in ornithology and
computer vision \cite{bangar2022vgg}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{images/VGG16.png}
    \caption{VGG16 architecture}%\label{fig:fig1}
\end{figure}
\newpage

\subsection{Max Pooling}

Max pooling is a down-sampling operation commonly used in convolutional neural
networks (CNNs) to reduce the spatial dimensions (height and width) of the
input feature maps. This reduction helps to decrease the computational load and
the number of parameters, which mitigates overfitting. Max pooling works by
sliding a window (typically of size 2x2) over the input feature map and
selecting the maximum value within each window. This operation retains the most
prominent features while discarding the less significant ones.

The equation for max pooling can be expressed as follows:

\[
    y(i, j, k) = \max_{(m, n) \in W} x(s \cdot i + m, s \cdot j + n, k)
\]

where:
\begin{itemize}
    \item \( y(i, j, k) \) is the output value at position \((i, j)\) in the
          \(k\)-th channel.
    \item \( x \) is the input feature map.
    \item \( W \) is the pooling window.
    \item \( s \) is the stride of the pooling window.
    \item \( m \) and \( n \) iterate over the dimensions of the pooling
          window.
\end{itemize}

\subsection{Softmax}

The softmax function is often used in the final layer of a neural network for
classification tasks. It converts the raw scores (logits) from the neural
network into probabilities, enabling a probabilistic interpretation of the
output. Each class's probability is proportional to the exponent of its
corresponding logit, normalized by the sum of the exponents of all logits.

The equation for the softmax function is given by:

\[
    \sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\]

where:
\begin{itemize}
    \item \( \sigma(z_i) \) is the softmax output for the \(i\)-th class.
    \item \( z_i \) is the logit (raw score) for the \(i\)-th class.
    \item \( K \) is the total number of classes.
\end{itemize}

\subsection{Algorithms Used}

\subsubsection{Data Augmentation}

% Data augmentation is a technique used to increase the diversity of training
% data without collecting new data. It involves applying random transformations
% such as rotation, translation, flipping, and scaling to the original dataset.
% This helps improve the robustness and generalization ability of machine
% learning models.

\begin{algorithm}
    \caption{Data Augmentation}
    \begin{algorithmic}[1]
        \FOR{each image in the training dataset}
        \STATE Apply random transformations such as:
        \begin{itemize}
            \item Horizontal/vertical flip
            \item Random rotation
            \item Zoom in/out
            \item Shear transformation
            \item Color jitter
        \end{itemize}
        \STATE Add the augmented image to the training dataset
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsubsection{Normalization}

% Normalization is a preprocessing step used to scale the pixel values of images
% to a specific range, typically [0, 1]. This helps in speeding up the
% convergence by reducing the variability in input data, making the optimization
% landscape smoother.

\begin{algorithm}
    \caption{Normalization}
    \begin{algorithmic}[1]
        \FOR{each image \(x\) in the dataset}
        \FOR{each pixel value \(x_{i,j}\)}
        \STATE Normalize the pixel value:
        \[
            x_{i,j} = \frac{x_{i,j} - \min(x)}{\max(x) - \min(x)}
        \]
        \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsubsection{Stochastic Gradient Descent (SGD)}

% Stochastic Gradient Descent is an iterative optimization algorithm used to
% minimize the loss function in machine learning models, especially neural
% networks. Unlike batch gradient descent, which uses the entire dataset, SGD
% updates the model parameters using a single training example or a mini-batch at
% each iteration. This approach provides faster convergence and is especially
% useful for large datasets.

\begin{algorithm}
    \caption{Stochastic Gradient Descent (SGD)}
    \begin{algorithmic}[1]
        \STATE Initialize the model parameters (weights and biases)
        \FOR{each epoch}
        \STATE Shuffle the training data
        \FOR{each training example \((x_i, y_i)\)}
        \STATE Compute the prediction \(\hat{y}_i\) using the current model
        parameters
        \STATE Calculate the loss \(L(\hat{y}_i, y_i)\)
        \STATE Compute the gradient of the loss with respect to the model
        parameters
        \STATE Update the model parameters using the gradient and the learning
        rate \(\eta\):
        \[
            \theta = \theta - \eta \nabla L(\hat{y}_i, y_i)
        \]
        \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\section{Mapping Location of Bird in Map}

Mapping the location of a bird in an application using
\texttt{google\_maps\_flutter} and \texttt{geolocator} involves using the
classified bird data to
pinpoint its location on a map. This is particularly useful for bird watchers
and researchers to track bird sightings.

\begin{algorithm}
    \caption{Mapping Location of Bird in Map}
    \begin{algorithmic}[1]
        \STATE Initialize the Google Maps widget
        \STATE Capture the image of the bird
        \STATE Get the classified bird species and captured location.
        \STATE Convert the location data to latitude and longitude coordinates
        \STATE Add a marker to the map at the specified coordinates
        \STATE Update the map view to center on the new marker
    \end{algorithmic}
\end{algorithm}
\newpage
\section{System Diagram}
The usecase diagram for Bird species identification from Audio and Image is
given
below:
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{images/usecase.png}
    \caption{Usecase Diagram for
        FeatherFind}%\label{fig:fig1}
\end{figure}

% \begin{figure}[h!]
%      \centering
%         \includegraphics[scale=0.7]{output/activity.jpg}
%         \caption{Activity Diagram}%\label{fig:fig1}
%      \end{figure}
%      \newpage
% \newpage
\newpage
\section{Software Development Model}
This project will be using an incremental methodology since it offers a
functioning prototype at an early stage of development. The requirements and
scope of the project can be altered as necessary by studying the prototype. The
rationale for the preference for this software development strategy is the
flexibility offered by adopting the incremental technique. In this paradigm,
the project goes through several releases or iterations prior to its official
release.
\begin{figure}[h!]
    % \centering
    \includegraphics[scale=0.25]{images/SDLC.png}
    \caption{Incremental Model for development of
        FeatherFind}%\label{fig:fig1}
\end{figure}
\newpage

\chapter{Epilogue}
The expected output of FeatherFind designed using \texttt{Figma} are displayed
below:
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.6]{images/onboarding_mockup.png}
    \caption{Onboarding Screen of
        FeatherFing}%\label{fig:fig1}
\end{figure}
\begin{figure}[h!]
    \centering
    \begin{minipage}[b]{0.4\linewidth}
        \includegraphics[width=\linewidth]{images/homepage.png}
        \caption{Homepage}
    \end{minipage}
    \hspace{1cm}
    \begin{minipage}[b]{0.4\linewidth}
        \includegraphics[width=\linewidth]{images/RecordingPage.png}
        \caption{Recording Page }
    \end{minipage}
\end{figure}
\begin{figure}[h!]
    \centering
    \begin{minipage}[b]{0.4\linewidth}
        \includegraphics[width=\linewidth]{images/CapturePage.png}
        \caption{Capture Page}
    \end{minipage}
    \hspace{1cm}
    \begin{minipage}[b]{0.4\linewidth}
        \includegraphics[width=\linewidth]{images/Identifiedimage.png}
        \caption{Classified Page}
    \end{minipage}
\end{figure}
\newpage

%Reference
\renewcommand\bibname{REFERENCES} % Change heading to References
\bibliographystyle{IEEEtran} % to use IEEE Format for referencing
\addcontentsline{toc}{chapter}{References} % to add references in TOC
\bibliography{library} % specify the .bib file containing reference information 

%Comment this Chapter if you do not need to include Appendix.

%\addcontentsline{toc}{chapter}{Appendix}

\end{figure}

\end{document}
