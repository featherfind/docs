\include{KECReportFormat} %includes the file KecReportFormat.tex that include all  necessary formattings
\usepackage{amsmath}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Define Macros for Details of your Project
\newcommand{\project}{Major Project } %Specify "Major Project" or "Minor Project"
\newcommand{\projectTitle}{BIRD SPECIES IDENTIFICATION FROM AUDIO AND IMAGE USING DCNN} %specify "Title" of Your Project
\newcommand{\doc}{Proposal} % specify the document you are preparing eg. "Proposal", "Mid-Term Report" or "Final Report"32
% Note that You have to sibmit "Final Report" for Pre-final defense as well.
\newcommand{\subCode}{CT654} %specify Subject of Your Project
\newcommand{\degree}{Bachelor in Computer Engineering} %specify your degree
\newcommand{\submittedBy}%Specify Names and Roll/Symbol Numbers of the Project Group Members
{
%Edit Member Names and Roll/Symbol No. and adjust width (\makebox[width]) if necessary 
\makebox[10cm]{Gaurav Giri \hfill [Kan077bct034]}\\
\makebox[10cm]{Iza K.C. \hfill [Kan077bct039]}\\
\makebox[10cm]{Prajwal Khatiwada\hfill [Kan077bct56]}\\
\makebox[10cm]{Samrat Kumar Adhikari \hfill [Kan077bct74]}\\
%\makebox[9cm]{Member Name \hfill [Roll/Symbol No.]}\\
} % Note that You must write your "Symbol Numbers"(Exam Roll Numbers) for Final Defenses

\newcommand{\submittedTo}{Department of Computer and Electronics Engineering} %specify your department
\newcommand{\hod}{Er. Rabindra Khati \\ Associate Professor}
 %specify Head ot the department
\newcommand{\defYear}{2024} %Defense Year
\newcommand{\defMonth}{June} %Defense Month- January, February, ...
\newcommand{\defDay}{5} %specify Defense Day- 1, 2, ...

\newif\ifhassupervisor
\hassupervisorfalse % to display supervisor name use command- \hassupervisortrue
 
\newcommand{\supervisor}{Er. Bishal Thapa}
\newcommand{\degSup}{Project Coordinator\\ Department of Computer and Electronics Engineering}
\newcommand{\external}{External's Name}
\newcommand{\degExternal}{Designation of External\\Second line if required} %Specify Name of External for Major Project (Required for Black Book) , use multiple lines (\\) if necessary


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Define Abberviations and Symbols
% NOTE that Only those Abberviations and Symbols that are included in document(using command \ac{}) will be displayed in the List of Abberviations and Symbols.

%class 'abbr': for List of Abbreviations



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% class `symbol': for List of Symbols
%\DeclareAcronym{transparencyFactor}{
%  short = \ensuremath{\alpha} ,
 % long  = Transparency Factor ,
 % sort  = Transparency Factor , % string to compare for sorting symbols... default string is the acronym name -"transparencyFactor"
  %class = symbol
%}% declares acronym named "transparencyFactor". Use \ac{UN} for short and \acl{UN} for long form.

%\DeclareAcronym{areaOfTriangle}{
 % short = \ensuremath{a} , % use \ensuremath{a} instead of $a$
 %% long  = Area of Triangle ,
 % sort  = Area of Triangle , % string to compare for sorting symbols
  %class = symbol
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%The Document
\begin{document}

\KECcoverpage
\KECtitlepage

\pagenumbering{roman}
%\KECapproval
% \chapter*{Acknowledgment}
% \addcontentsline{toc}{chapter}{Acknowledgment}%to include this chapter in TOC
% We would like to express our gratitude to everyone who helped us to complete this project.
% First and foremost, we would like to acknowledge the crucial role of our teachers of Department of Electronics and Computer Engineering for their guidance, support, and feedback throughout the project. Next, we would like
% to give our gratitude to our classmates, for providing constructive feedback and engaging in
% thought-provoking discussions regarding our project. Next, weâ€™d like to thank all lecturers
% of our department for guiding us through the beginning of our project to the end. Finally,
% a special gratitude to our family and friends, for their love, encouragement, and support
% throughout our academic journey.\\
% Thank you all for your invaluable contributions to this project.\\
% \makebox[10cm]{Ajaya Chaudhary \hfill }\\
% \makebox[10cm]{Gaurav Giri \hfill }\\
% \makebox[10cm]{Iza K.C. \hfill }\\
% \makebox[10cm]{Lakesh Shrestha \hfill }
%to display members name under Acknowledgement
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}%to include this chapter in TOC 
SnapTag introduces an approach for efficiently analyzing handwritten documents by
leveraging image processing techniques and Named Entity Recognition (NER). The
primary objective is to develop a system capable of extracting meaningful information
from handwritten content provided by users and subsequently generating relevant tags for
improved document organization and categorization.\\\\
SnapTag employs image processing methods such as image binarization, thresholding,
denoising to enhance the quality of scanned handwritten documents. Through these
techniques, the system effectively preprocesses images, mitigating noise and improving
the clarity of handwritten text. Then, the methodology involves the integration of Canny
edge detection and Hough line transformation, coupled with K-means clustering, to
accurately detect document boundaries. Subsequent stages of the process incorporate
image segmentation to isolate words and characters, followed by a classification model
that identifies each character within the document. The character recognition phase utilizes a trained classification CNN model, to accurately
classify individual characters into predefined classes. This step is crucial for deciphering
the handwritten content and preparing it for further analysis.In the final stage, NER is
employed to extract meaningful tags from the processed document providing valuable
metadata that enhances the document's categorization and searchability.

{\textit{Keywords$-$Optical Character Recognition, Binarization, Thresholding, Denoising, Boundary Detection, Hough Line Transformation, K-Means Clustering, Convolutional Neural Network, Named Entity Recognition}} 

%{\par
%\begin{flushright}
%\vskip -20pt
%\setstretch{1.2}
%\submittedBy
%\end{flushright}}

%to adjust spacings for TOC, LOF, LOT
{

%TOC, LOF and LOT
\KECadjusttocspacings % defined in KECReportFormat.tex to adjust spacings
\makeatletter
% to add vskip of 18 point which is reduced when parskip is set to 0 in \LECadjustspacings
\def\@makeschapterhead#1{%
  %\vspace*{50\p@}% Remove the vertical space
  {\newpage \parindent \z@ \raggedright
    \normalfont
    \interlinepenalty\@M
    \center\fontsize{16pt}{1} \bfseries \MakeUppercase{#1}\par\nobreak
    \vskip 18\p@ % adjust space after heading 18pt
  }}
\makeatother 



\tableofcontents % prints table of contents
\listoffigures % prints list of figures
\addcontentsline{toc}{chapter}{List Of Figures}
%\listoftables % prints list of table
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%comment this chapter if you don't have List of Abbreviations
%\KECloa % defined in KECReportFormat

%comment this chapter if you don't have List of Symbols
%\KEClos % defined in KECReportFormat


\chapter*{Abbreviations}
\addcontentsline{toc}{chapter}{Abbreviations}
\begin{tabbing}
\hspace{50mm}\=\kill
CNN \>  Convolutional Neural Network \\
EMNIST \> Extended Modified National Institute of Standards and Technology\\
NER \> Named Entity Recognition\\
OCR \> Optical Character Recognition\\
PDF \> Portable File Document\\
ReLU \> Rectified Linear Unit\\
% \DeclareAcronym{CNN}{
%  short = \ensuremath{a} , % use \ensuremath{a} instead of $a$
%  long  = Convolutional Neural Network,
%  sort  = Convolutional Neural Network, % string to compare for sorting symbols
%   class = symbol
% }
% \DeclareAcronym{EMNIST}{
%  short = \ensuremath{a} , % use \ensuremath{a} instead of $a$
%  long  = Extended Modified National Institute of Standards and Technology,
%  sort  = Extended Modified National Institute of Standards and Technology, % string to compare for sorting symbols
%   class = symbol
% }
\end{tabbing}
\newpage
\pagenumbering{arabic}

\chapter{Introduction}
\section{Background}\label{sec:bkgrnd}%label your section if you require to refer them somewhere else in your document.
Globally, the avian kingdom is vast, with over 11,000 species, a testament to nature's complexity and evolutionary prowess. This figure, sourced from the International Ornithological Committee as of April 2023, merely scratches the surface of avian diversity, each species a unique entity with its own ecological role and evolutionary story.\cite{ioc_updates}

Turning our gaze to Nepal, a country of remarkable biodiversity and varied ecosystems, from the lowland Terai to the towering Himalayas, it is home to more than 887 bird species, as reported by the Himalayan Nature organization. This represents more than 8\% of the world's known bird species, a significant figure given Nepal's relatively small geographical footprint.\cite{himalayan}

Among these, a number are endangered, their existence threatened by habitat loss, climate change, and human activities. The National Red List of Nepal's Birds, a comprehensive assessment of the country's avian biodiversity, identifies several species at risk. Specifically, Nepal harbors 168 nationally threatened bird species, including 68 Critically Endangered, 38 Endangered, and 62 Vulnerable species, as detailed in a publication by the Journal of Threatened Taxa.\cite{inskipp2017nepala}

The plight of these endangered species underscores the urgency of conservation efforts. Technologies such as audio recognition and image classification offer innovative tools for identifying and monitoring bird populations. By analyzing the unique sounds and visual characteristics of birds, researchers can enhance our understanding of species distribution, behavior, and threats. Such technologies not only aid in the conservation of endangered species but also contribute to the broader field of biodiversity research.

  
\section{Problem Statement}
In Nepal, a hotspot of avian biodiversity, accurately identifying and classifying bird species, particularly those that are endangered, is a critical yet complex task. Traditional observation methods are limited by the vast geographical and ecological diversity of the region, making it challenging to monitor and protect these birds effectively. The necessity for precise identification is paramount for conservation efforts aimed at maintaining ecosystem balance. To address this, there is a pressing need for a method that can overcome these constraints by leveraging advanced technologies capable of distinguishing between the myriad of bird calls and songs, as well as visual markers through image classification. Such a method promises to automate the identification process, enhancing accuracy and efficiency in monitoring endangered species.

Implementing audio recognition and image classification technologies, however, raises several challenges, including the effective training of these systems to recognize the specific calls and visual markers of Nepal's endangered birds. This requires collecting and curating extensive datasets, a task complicated by the elusive nature of many species and the complex acoustics of their habitats. Additionally, integrating these technologies into conservation strategies is crucial for not just identifying but also protecting these species from various threats. A multidisciplinary approach, combining expertise in ornithology, conservation biology, machine learning, and environmental science, is essential to develop a robust system. Such a system could provide reliable data on species distribution, population trends, and habitat use, thereby informing targeted conservation actions and policies to preserve Nepal's rich avian biodiversity.

\section{Objectives}
\begin{enumerate}[label=\roman*]
    \item To develop and implement an integrated technological solution that utilizes advanced audio recognition and image classification techniques for the accurate identification and monitoring of bird species in Nepal, with an emphasis on endangered species.
\end{enumerate}

\section{Application Scope}
 \begin{enumerate}
    \item \textbf{Conservation Efforts:}\\This system will enhance conservation by enabling accurate monitoring 
    of bird populations, helping track endangered species and take a step towards habitat protection.
    \item \textbf{Biodiversity Monitoring:}\\Automated identification will aid biodiversity monitoring by 
    processing large datasets, helping detect species distribution on bird communities.
    \item \textbf{Ecological Research:}\\ Researchers can use the system to study bird migration,
     and habitat use, providing crucial data for modeling ecosystems and understanding ecological interactions.
    \item \textbf{Environmental Education and Awareness:}\\Integrated into educational programs, this tool will 
    raise public awareness about biodiversity and conservation, engaging students and citizen scientists in bird identification.
    \item \textbf{Bird viewing:}\\Bird enthusiasts will benefit from these systems as they will enhance bird watching experiences by providing instant identification of bird species
 \end{enumerate}
 \section{Features}
 \begin{enumerate}
    \item \textbf{Species Identification Using Audio:}\\
    The app allows users to record bird sounds in real-time using their device's microphone or 
    upload pre-recorded audio files. Advanced noise filtering techniques isolate bird calls from 
    background noise, and sound wave analysis helps in identifying distinct frequency patterns. 
    Machine learning algorithms, trained on a vast database of bird calls, match the recorded 
    sound to identify the bird species accurately.
    
    \item \textbf{Species Identification Using Image:}\\
    Users can capture photos of birds using the app's camera or upload images from their gallery.
    The app enhances image quality and analyzes features such as color, size, shape, and patterns. 
    Utilizing computer vision models, the app identifies the bird species by comparing the image 
    with a comprehensive database of bird images.
    
    \item \textbf{Mapping Identified Bird Habitat:}\\
    The app tags the location of identified birds using GPS, providing detailed habitat information 
    typical of each species. Integrating with mapping services, it displays bird sightings on an 
    interactive map, generating heat maps to show species density and distribution. Additionally, 
    it tracks and visualizes bird migration patterns over time, helping users understand seasonal
    movements.
    
    \item \textbf{Provide Description About the Birds:}\\
    For each identified bird species, the app offers detailed profiles that include scientific and 
    common names, physical descriptions, and conservation status. It also provides audio and visual 
    media for reference, along with information on the bird's behavior, diet, and typical habitats, 
    enriching the user's understanding of the species.
    
\end{enumerate}
 \section{Feasibility Study}
Before implementation of project design, the feasibility analysis of the project must be
done to move any further. The feasibility analysis of the project gives an idea on how the
project will perform and its impact in the real world scenario. So, it is of utmost
importance.
 \subsection{Economic Feasibility}
Our system is economically achievable as a result of the development of several tools,
libraries, and frameworks. Since all the software required to construct it is free and
readily available online, this project is incredibly cost-effective. Only time and effort are
needed to create a worthwhile, genuinely passive system. The project doesn't come at a
substantial cost. From an economic standpoint, the project appears successful in this
sense.

 \subsection{Technical Feasibility}
 The software needed to implement a project can be downloaded from a wide variety of
online resources. Technically speaking, the project is feasible as the necessary software is
easily available. We were able to learn the information we required for the project
through a variety of online sources, including classes. All the libraries and data are
accessible online for free because this project does not require any licensing costs. It is
technically possible if one has the necessary information and resources.

 \subsection{Operational Feasibility}
 Our project intends to improve hand-written notes taking experiences with a primary
focus on searching keywords in notes/documents to make it more accessible. The project
is intended for a sizable group of learners and students who desire to maintain their
handwritten notes. Consequently, our project is operationally feasible.

 \subsection{Schedule Feasibility}
 The workload of the project is divided amongst the project members. The scheduling is
 done according to an incremental model where different modules are planned to be
 assigned to the group members.So, the project fulfills the schedule feasibility
 requirements.
 %(GAntt chart here)
% \newpage
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.35]{images/GanttChart.png}
    \caption{Gantt Chart}\label{fig:my_label}
\end{figure}
\section{System Requirements}

\subsection{Development Requirements}
\begin{table}[h]
    \centering
    \caption{Development Requirements}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Software Requirements} & \textbf{Hardware Requirements}\\ \hline
        Programming Language: Python,Dart, Java & Camera: \(>\)= 12 Megapixels\\ \hline
        Design Tools: Figma & RAM: \(>\)= 8 GB\\ \hline
        OpenCV Library, spaCy & CPU: i5 10th (Recommended)\\ \hline
        Frameworks: Flutter, Tensorflow & GPU: P100\\ \hline
        Testing and Debugging Tools & Storage: \(>\)= 10 GB\\ \hline
    \end{tabular}
\end{table}
\newpage
\subsection{Deployment Requirements}
\begin{table}[ht]
    \centering
    \caption{Deployment Requirements}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Software Requirements} & \textbf{Hardware Requirements}\\ \hline
        Android: \(>\)= 10 & Camera: \(>\)= 12 Megapixels\\ \hline
        Read/Write FileSystem& RAM: \(>\) 4 GB\\ \hline
        Internet Accessibility & Storage: \(>\)= 5 GB\\ \hline
    \end{tabular}
\end{table}


\chapter{Literature Review}

Regardless of whether they are documented or not, every project has helped to shape the world as it is today.
Other researchers can benefit from documented projects by learning specifics about problems and how to solve
them. Additionally, they boostproject efficiency by removing the need to start the project from the beginning 
and specifying the starting point.

\section{Related Works}
BirdNET is a cutting-edge research platform developed through collaboration between the K. Lisa Yang Center for Conservation Bioacoustics at the Cornell Lab of Ornithology and the Chair of Media Informatics at Chemnitz University of Technology. Its primary aim is to detect and classify bird sounds using machine learning technologies, serving both experts and citizen scientists in their efforts to monitor and protect bird populations.\\\\
BirdNET can identify around 3,000 of the world's most common bird species, with plans to expand this number. Features such as a live submissions map and a Twitter bot are included to engage the community and share real-time data. The project is supported by donations and collaborations, offering opportunities for researchers and developers to contribute to its growth. BirdNET serves as an invaluable tool for bird enthusiasts, conservationists, and biologists alike, providing innovative solutions for large-scale acoustic monitoring and contributing to the conservation and understanding of avian biodiversity.\\\\
The BirdCLEF 2023 competition on Kaggle is a significant data science challenge that falls under the broader LifeCLEF initiative, aimed at pushing the boundaries of species identification and biodiversity monitoring through technological innovation. This particular competition focuses on the development of machine learning models that can identify bird species based on audio recordings. It presents a complex and realistic challenge due to the diversity of the audio recordings, which are collected from various environments and feature a wide range of bird species.
\section{Related Research}

The research paper `Audio Classifier for Automatic Identification of Endangered Bird Species of Nepal' focuses on developing an audio classifier to identify endangered bird species in Nepal using deep learning techniques. The dataset, collected from xeno-canto.org, comprises 2215 audio recordings of 41 bird species, 38 of which are endangered. This dataset was expanded to 6733 recordings through 10-second audio splitting and Gaussian noise augmentation, with 5407 recordings used for training, 639 for validation, and 687 for testing. The methodology involved handling imbalanced class distribution through data augmentation, employing Mel spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs) for feature extraction, and developing a custom Convolutional Neural Network (CNN) model and an EfficientNet model. The hyperparameters of these models were optimized using a genetic algorithm. The Mel spectrograms were created using Short-Time Fourier Transform, converting amplitudes to decibel scale, and applying Mel filter banks to the spectrograms. Similarly, MFCCs were derived by framing the audio signals, applying Discrete Fourier Transform, logarithmic scaling, Mel scaling, and Discrete Cosine Transform. The EfficientNet architecture utilized compound scaling for network depth, width, and resolution. The findings indicated that the proposed approach achieved satisfactory results in classifying the bird species. However, limitations include the relatively small dataset size and the need for further enhancement in model robustness and accuracy. Future enhancements could involve expanding the dataset, exploring additional feature extraction techniques, and incorporating more advanced deep learning models to improve classification performance. This research contributes significantly to the conservation efforts by providing a reliable method for automatic bird species identification, aiding in monitoring and protecting endangered species.\cite{gautam2023audio}\\

The paper `Audio Bird Classification with Inception-v4 extended with Time and Time-Frequency Attention Mechanisms' presents an innovative adaptation of the Inception-v4 deep convolutional network for bioacoustic classification, focusing specifically on bird sound recognition. The datasets employed include various bird sounds, prominently from the BirdClef2017 challenge, consisting of 1500 bird species recordings. The methodology revolves around treating bird sound classification as an image classification problem through transfer learning. The Inception-v4 model, initially pre-trained on ImageNet, was adapted to process time-frequency representations of bird sounds by converting these sounds into RGB images using three log-spectrograms generated via fast Fourier transform at different scales (128, 512, 2048 bins). Data augmentation techniques, common in computer vision, were applied to these spectrograms to enhance the robustness of the model. The findings demonstrate that the model, termed `Soundception', integrates time and time-frequency attention mechanisms effectively, significantly improving classification accuracy. The results highlight Soundception's outstanding performance, achieving a mean average precision (MAP) of 0.714 in classifying 1500 bird species, 0.616 MAP for background species, and 0.288 MAP for soundscapes with time-codes, making it the top model in the BirdClef2017 challenge across multiple tasks. However, limitations include the incomplete convergence of the model due to computational constraints and the extensive GPU resources required for training, which restricted the full potential realization within the challenge's timeframe. The paper concludes with a discussion on future improvements, such as exploring different scalable optimizations and incorporating stacked GRU layers for better audio-to-image representation learning, underscoring the potential of transfer learning from advanced image classification models to acoustic domains.\cite{sevilla2017audio}\\

The research paper `Bird Species Identification using Deep Learning' presents a methodology using Deep Convolutional Neural Networks (DCNNs) to classify bird species, leveraging the Caltech-UCSD Birds 200 (CUB-200-2011) dataset, which contains 11,788 annotated images of 200 bird species. The methodology involves converting images to grey scale to reduce computational complexity, followed by the application of DCNNs using TensorFlow to extract hierarchical features from images such as edges, textures, and complex patterns. The neural network architecture includes convolutional layers for feature extraction, pooling layers for dimensionality reduction, activation layers for non-linearity, and fully connected layers for classification. Key findings show the DCNN model achieved a testing accuracy of 80\% and training accuracy of 93\%, with a validation accuracy of around 75\%, indicating robust performance across different data splits. The study highlights the effectiveness of combining multiple features (head, body, color, beak) over single-feature classification, with generated autographs and score sheets facilitating identification. The system's usability is enhanced through a web interface for image uploads, and future directions propose mobile app development and cloud integration to improve accessibility and scalability. However, the research also identifies limitations such as the dependency on the quality and diversity of the dataset, potential overfitting due to the complexity of the model, and the computational resources required for training deep learning models. Addressing these limitations could further enhance the model's accuracy and applicability in real-world scenarios.\cite{Gavali2019Bird}\\

The research paper `Bird species classification from an Image using VGG-16 Network' utilizes machine learning and deep learning techniques has shown significant advancements in both methodology and accuracy. The primary focus has been on the creation and utilization of diverse datasets, with a notable example being the dataset of 1600 images across 27 bird species, as well as the Caltech-UCSD Birds-200-2011 Dataset. Methodologically, the use of Convolutional Neural Networks (CNNs), particularly the VGG-16 model, has been prevalent due to its effectiveness in feature extraction from images. This approach has been complemented by transfer learning techniques, which have notably improved classification accuracies, as seen in the use of pre-trained networks like AlexNet and VGG-16, achieving accuracies up to 85.4\% and 92.13\% respectively. The application of machine learning algorithms such as SVM with a linear kernel and KNN has also been explored, with SVM achieving an accuracy of 89\% after parameter optimization. Additionally, the integration of computer vision methods to extract Histogram Oriented Gradients and RGB histogram values has further enhanced classification performance. Despite these advancements, the studies have encountered limitations, including the challenge of accurately classifying bird species from various angles and positions, and the need for extensive preprocessing to remove noise from datasets. Overall, the body of work demonstrates a trend towards higher accuracy in bird species classification through the innovative use of deep learning techniques and sophisticated feature extraction methods, though challenges remain in dealing with the variability of natural images and the computational demands of processing large datasets.\cite{islam2019bird}\\

This research paper presents significant advancements in the field of `Automatic bird species identification through the integration of audio signal processing and neural networks'. The study, conducted by Chandu B, Akash Munikoti, Karthik S Murthy, Ganesh Murthy V, and Chaitra Nagaraj from the BNM Institute of Technology, outlines a robust methodology for identifying bird species from audio recordings, leveraging a combination of meticulously curated datasets and sophisticated machine learning techniques. The dataset, a critical component of the study, was manually compiled from both local recordings and online resources such as xeno-canto.com, featuring audio clips from species like cuckoo, sparrow, crow, and laughing dove, as well as ambient noise and human voices to simulate real-world conditions. Pre-processing techniques including pre-emphasis, framing, silence removal, and reconstruction were applied to the audio clips to enhance the relevant frequency components and eliminate unnecessary noise, ensuring the purity of the dataset. Spectrograms of these processed clips were generated and used as input for training a convolutional neural network (CNN), specifically AlexNet, chosen for its high accuracy in image classification tasks. Through transfer learning, AlexNet was adapted to recognize bird species from the spectrograms, achieving a classification accuracy of 97\% in controlled environments. However, recognizing the variability of real-world conditions, the researchers retrained the model with datasets containing ambient noise, achieving a real-time classification accuracy of 91\%. Despite these promising results, the study acknowledges limitations such as the relatively small size of the dataset and the need for further tuning of performance parameters to improve robustness. The potential for future applications is vast, including the development of mobile applications and hardware implementations for ecological monitoring, highlighting both the scientific and practical significance of automated bird species identification systems.\cite{chandu2020automated}\\

The paper `An Ensemble of Convolutional Neural Networks for Audio Classification' delves into a comprehensive study on CNN classification using different architectures, data augmentation techniques, and audio signal representations, aimed at enhancing audio classification tasks across various datasets. The study employs three datasets: BIRDZ, CAT, and ESC-50, each offering unique challenges in audio classification. The methodology involves training five convolutional neural networks (CNNs) with four audio representations combined with six different data augmentation methods, resulting in thirty-five subtypes of ensembles. The audio representations include techniques such as the Discrete Gabor Transform (DGT), Waveform Similarity OverLap Add (WSOLA), and Phase Vocoder. The data augmentation methods encompass procedures like short spectrogram augmentation, random time shift, and frequency masking. The CNN architectures are pre-trained models fine-tuned with these augmented datasets to boost classification accuracy. The findings reveal that the ensemble method outperforms standalone networks, achieving 97\% accuracy on the BIRDZ dataset, 90.51\% on the CAT dataset, and 88.65\% on the ESC-50 dataset. The study also highlights that the best-performing CNNs are VGG16 and VGG19, with DGT as the most effective signal representation. However, the study acknowledges limitations, such as the computational cost of training ensembles and the variability in performance across different augmentation techniques. Notably, no single augmentation protocol consistently outperforms others across all datasets. The results underscore the potential of combining different CNNs and augmentation policies to enhance performance, although this approach demands significant computational resources. Additionally, the study provides the MATLAB source code for reproducibility, contributing to the research community's efforts in advancing audio classification technologies.\cite{nanni2021ensemble}\\

The literature review focused on the `analysis of bird call datasets sourced from Xeno-Canto', comprising 72,172 samples from 264 bird species in 16-bit wav format with a 16 kHz sampling rate. The methodology involved preprocessing the audio data to filter out low-frequency noise and normalize signal amplitude, followed by generating Mel-spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs) as inputs for deep learning models. The Mel-spectrograms were produced using discrete Fourier transform (DFT), and the MFCCs were derived by applying discrete cosine transform (DCT) to the Mel-spectrogram. The study employed various metrics to evaluate the performance of these methods, including ROC analysis to visualize model effectiveness. Findings indicated that the proposed models showed significant promise in identifying bird species from their calls, with improvements in classification accuracy compared to previous approaches. However, limitations were noted, including potential biases in the dataset due to uneven sample distribution across species and the challenge of background noise affecting signal quality. Future work suggested enhancing noise reduction techniques and exploring more sophisticated neural network architectures to further improve model robustness and accuracy.\cite{wang2022efficient}\\

The study conducted an in-depth analysis of `bird species recognition through acoustic monitoring', utilizing a robust dataset of bird sound samples, meticulously annotated and validated for accuracy. The dataset, referred to as SD, comprises multispecies bird sound recordings, each labeled with species name and sample ID, along with corresponding metadata, providing a comprehensive foundation for model training and evaluation. Methodologically, the research employed a spectrogram-based feature extraction approach, leveraging Short-Time Fourier Transform (STFT) to capture the intricate temporal and spectral characteristics of bird sounds. This was followed by the application of a Multilayer Perceptron (MLP) classifier to distinguish between different bird species. The findings reveal that the proposed model achieved high recognition accuracy, with some species being identified with perfect precision, recall, and accuracy (100\%), though the performance varied across species, with a few showing lower recognition rates (86.9\%) and precision/recall values ranging between 50-75\%. The results demonstrated an overall classification accuracy of 96\%, with cross-validation accuracy standing at 81.4\%, highlighting the model's robustness yet indicating room for improvement in generalizability across diverse datasets. Despite the promising results, the study acknowledges several limitations, including the variability in recognition accuracy among different species and the potential influence of environmental noise on model performance. Future work is suggested to explore feature and model fusion techniques, integrate the model with cloud-based systems for real-time recognition, and expand the dataset to include a broader range of bird species to enhance the model's applicability and accuracy in practical scenarios.\cite{pahuja2021soundf}\\
\\
\chapter{Methodology}
\section*{}
\begin{figure}[h]
    \centering
        \includegraphics[scale=0.46]{images/Methodology.png}
        \caption{Block diagram for the working mechanism of the system}%\label{fig:fig1}
    \end{figure}
\newpage
\section{Working Mechanism}
The methodology diagram is shown above fig no. 3.1 which consists of several stages such as image acquisition i.e.testing image, pre-processing (grayscale conversion, binarization, document detection), segmentation (lines, words, and characters), feature extraction, and classification using CNN layers into\cite{Ishfar2020DocumentDetection} character classes.
\subsection{Image Acquisition}
The first step is to obtain a handwritten script through  scanning physical documentation or images using a camera.
\subsection{Pre-processing}
It is essential to enhance the quality and prepare the image for further analysis. This step includes Grayscale Conversion, Binarization, Noise Removal and skew correction. Grayscale conversion is used to reduce complexity of handling multiple color channels ( 24-bit to 8-bit). Binarization is used to convert grayscale images into black and white to make it easier for segmentation.\newline
Noise removal is used to remove unwanted variations in pixel intensity that degrade image quality. The proposed method uses a median filter to efficiently remove salt and pepper noise. This filter sorts adjacent pixel values and replaces the current pixel value with the median, reducing the noise and improving image.
\subsection{Document Detection}
The document detection is used for auto-cropping the image and correcting the perspective angle of the page of interest. For that quadrilateral points are obtained through the merged process of canny edge detection and hough transformation then KMeans Clustering that clusters the intersection points into 4 groups which are the quadrilaterals. Then through the quadrilaterals, perspective transformation is performed to get a good perspective of the page\cite{Ishfar2020DocumentDetection}.
\subsection{Segmentation}
In the segmentation process, we first tackle Line Segmentation, where the input image is       analyzed to identify lines of text. By finding dark centroids in the image, we can mark the positions of each line. We employ a projection profile-based algorithm for this task and address the issue of skewed text by applying skew correction techniques.\newline 
Moving on to Word Segmentation, our approach involves morphological dilation, which allows us to connect individual characters in a line, ultimately helping us to extract separate words. Using labeling techniques, we efficiently isolate and extract words from the connected components.\newline 
Character Segmentation is a crucial step, particularly for cursive handwriting. To address the challenges of cursive characters, we implement skeletonization and vertical projection techniques to detect potential segmentation points. A distance-based approach is utilized to avoid over-segmentation, ensuring that characters like 'm', 'n', 'u', 'v', 'w', etc., are correctly identified. For untouched characters, a vertical projection is employed to identify spaces between characters, facilitating their separation.\newline
Through these segmentation processes, we break down the handwriting image into lines, words, and individual characters, which is essential for accurate and efficient handwriting recognition.
\subsection{Features Extraction}
Feature extraction involves identifying important image features and saving them for further processing. It bridges the gap between pictorial and data representation in image processing. The proposed method employs a Convolutional Neural Network (CNN) for effective feature extraction, making the process more efficient and accurate.
\subsubsection{Training of Convolutional Neural Network}
The configured layers of Convolution Neural Network are shown below. To train the neural network, the dataset is divided into two sets: the training data and the testing data. The complex handwritten character dataset has data format of 28x28 ".png" format picture. The data set has a total of 62 categories of 0-9, a-z and A-Z, corresponding to the files "0" to "61" in the order of "label.txt". The data set is divided into two parts, the unprocessed original data image is stored in the "0" to "61" in the "V0.3/data" folder, and the binarized data image Stored in "0" to "61" in the "V0.3/data-bin" folder\cite{Ishfar2020DocumentDetection}.
\begin{figure}[h]
    \centering
        \includegraphics[scale=0.45]{images/Data_overview.png}
        \caption{Dataset overview for the A good CHoiCe}%\label{fig:fig1}
    \end{figure}
    The EMNIST also has 62 character classes divided into digit classes and letter classes containing a total 814,255 samples\cite{Ishfar2020DocumentDetection}.
    \begin{figure}[h]

        \centering
            \includegraphics[scale=0.45]{images/EMNIST_Dataset.png}
            \caption{Visual representation of E MNIST dataset}%\label{fig:fig1}
        \end{figure}  
    \begin{figure}[h!]
        \centering
            \includegraphics[scale=0.45]{images/CNN_model.png}
            \caption{Configured layers of Convolution Neural Network}%\label{fig:fig1}
        \end{figure} 
    \newpage
\newpage
\subsubsection{Document Recognition} 
The neural network used for image recognition involves four primary operations: Convolution, Non-Linearity (ReLU), Pooling, and Fully Connected Layer.
\begin{enumerate}
    
    \item Convolution: The convolution operation extracts features from the input image M using a filter matrix N. The resulting feature map \( f_{\text{con}}(k,l)\) can be calculated as follows:
    \begin{eqnarray}
        f_{\text{con}} = \text{convolution}(m,n) 
    \end{eqnarray}
     \begin{eqnarray}
         (m\bigotimes n)(k,l) = \sum_{m} \sum_{n}    M(m,n)  N(k-m,l-n) \quad
     \end{eqnarray}   
        
    
    
    
    \item Non-Linearity (ReLU): The ReLU function is applied element-wise to the feature map to introduce non-linearity and obtain the rectified feature map frec(xk): 
    \begin{eqnarray}
    f_{\text{Rec}} = \text{ReLU}(x,l) = \max(0, x_k)  \quad
    \end{eqnarray}   

    \item Pooling: Pooling reduces feature map dimensionality while preserving important information. The Max pooling technique selects the maximum value from a defined spatial matrix (e.g. 2x2):
    \begin{eqnarray}
    f_{\text{pool}}(k,l) = \max(x_k)
    \end{eqnarray}   

    \item Fully Connected Layer: In this layer, all neurons are connected to each other, and the output is used for classification.\newline
    Finally, the Softmax Layer outputs probabilities for each class of the handwritten characters. The Softmax function calculates the probability for an input element xk. belonging to label i as follows:
    \begin{eqnarray}
        p(O_i) = \text{softmax}(x_k) = \frac{\exp(x_k)}{\sum_{g = 1}^{y}   \exp(x_k)} 
    \end{eqnarray}
    
    The Classification Layer then identifies the recognized character based on the highest probability assigned by the Softmax Layer.
\end{enumerate}
\subsection{Named Entity Extraction using spaCy}
NER is a process in which anything that is denoted by a proper name or tag,for example, a location, an organization, or a person is identified as an entity.Named entities include things like geographical location, date, time, or money, and customization of the NER model for user-defined named entities is also possible\cite{shelar2020named}.\newline
spaCy is a free, open source library that allows advanced Natural Language Processing in Python. This library will be used to extract the unique word which will be used as tags for the image while searching it. Unique words are generated through named entity recognition for which "en\_core\_web\_sm" is used, which is a linguistic model.\newline
\section{Algorithm Used}
\begin{itemize}
    \item \textbf{Line Segmentation:} The Line Segmentation algorithm starts with image padding to avoid segmentation errors. It computes the horizontal projection to identify text regions and detects dark lanes using a threshold in the binary image. The algorithm labels and counts dark regions to determine total lines of text. Centroids are calculated for precise positioning and stored for further processing. x and y coordinates of centroids are extracted. Text regions are cropped based on consecutive y-coordinates for accurate segmentation, enabling proper division of each handwritten script line for improved recognition and analysis.
    \item \textbf{Word Segmentation:} The Word Segmentation algorithm aims to extract individual words from previously segmented lines of handwritten text. It involves three steps: first, dilating the line image to expand the pixels and enhance visual distinction between connected components; second, labeling the connected components to identify distinct words; and finally, separating the labeled components to obtain individual images of each word. These separated images represent the segmented words of the handwritten text.
    \item \textbf{Cursive Character Segmentation:} The Cursive Character Segmentation algorithm separates connected characters in a handwritten text image by skeletonizing the image and computing its vertical projection. It marks segmentation points using the "Segmentation Points" (SP) array, starting from the first character. For each column and row, it checks if certain conditions are met to identify connected characters. If these conditions are fulfilled, the algorithm stores the column in the SP array based on a threshold distance. After marking the segmentation points, it draws red lines on the word image for visualization and stores 0 in the rows of SP to disconnect the characters. This process paves the way for further character segmentation and recognition.
    \item \textbf{Untouched Character Segmentation:} The Untouched Character Segmentation algorithm effectively segments individual characters in a handwritten text image that have not been separated previously. It begins by padding the image for better segmentation and computes the Vertical Projection (VP) to identify character zones. Applying a threshold to the binary image detects dark lanes between letters. Each dark region is labeled, and the total character count is determined. Centroids are calculated for precise character positioning, and their x and y coordinates are extracted. The algorithm then crops text regions assuming a line between consecutive x-coordinates, ensuring accurate character segmentation. Cropped regions are stored separately, facilitating proper segmentation for s subsequent recognition and analysis.
  \end{itemize}
  \newpage
\section{System Diagrams}
The usecase diagram and activity diagram for SNAPTAG are given below:
\begin{figure}[h!]
    \centering
        \includegraphics[scale=0.7]{output/usecase.png}
        \caption{UseCase Diagram}%\label{fig:fig1}
    \end{figure}
\newpage
\begin{figure}[h!]
     \centering
        \includegraphics[scale=0.7]{output/activity.jpg}
        \caption{Activity Diagram}%\label{fig:fig1}
     \end{figure}
     \newpage
\newpage
\section{Software Development Model}
This project is developed using an incremental methodology since it offers a functioning prototype at an early stage of development. The requirements and scope of the project can be altered as necessary by studying the prototype. The rationale for the preference for this software development strategy is the flexibility offered by adopting the incremental technique. In this paradigm, the project goes through several releases or iterations prior to its official release.
\begin{figure}[h!]
    % \centering
        \includegraphics[scale=0.45]{images/SDLC.png}
        \caption{Incremental Model for development of SnapTag}%\label{fig:fig1}
    \end{figure}
\newpage
\chapter{Result And Discussion}
%\addcontentsline{toc}{chapter}{Result And Discussion}
\section{Result}
We have completed the design and development of the system along with the required output of the project. The project currently allows user to register and log-in into the system. User can post queries, answer, vote, follow other users and search queries.  Trending section shows most relevant post using half life decay algorithm.
\section{Work Done}
\begin{enumerate}
    \item Preprocessing: Common processing steps like Thresholding, De-noising, Resizing were done. In threshold, we used Otsu's Binarization due to its automatic optimal threshold value determination and avoiding having to choose a value. In order to do so, the cv2.threshold() function was used. Processing high resolution images are quite resource intensive. So, images were scaled down for faster processing using cv2.resize() function. And we performed de-noising to remove noise from images. To do this, we used cv2.fastNlMeansDenoising() function because of its faster processing while maintaining the denoising performance.
    \item Opening and closing: Opening and Closing are used as morphological operations to manipulate the shapes and structures of objects within an image. Closing and opening was used to improve the quality of the input image for further process of edge detection by reducing noise,smoothing contours . 
    \item Hough line transformation: Hough transformation was used after using Canny edge detection as this edge description is commonly obtained using Canny may be noisy, i.e. it may contain multiple edge fragments corresponding to a single whole feature. Furthermore, the output of an edge detector like Canny defines only where features are in an image, the work of the Hough transform is to determine both what the features are and how many of them exist in the image. 
    \newpage
    \newpage
    \begin{figure}[h]
        \centering
        \begin{minipage}[b]{0.30\linewidth}
            \includegraphics[width=\linewidth]{output/original.jpg}
            \caption{Original image}
        \end{minipage}
        \hspace{3cm}
        \begin{minipage}[b]{0.30\linewidth}
            \includegraphics[width=\linewidth]{output/thresholded.jpg}
            \caption{Threshold Image}
        \end{minipage}
        
    \end{figure}
    \begin{figure}[h]
        \centering
        \begin{minipage}[b]{0.30\linewidth}
            \includegraphics[width=\linewidth]{output/closed.jpg}
            \caption{Closed Image}
        \end{minipage}
        \hspace{3cm}
        \begin{minipage}[b]{0.30\linewidth}
            \includegraphics[width=\linewidth]{output/edges.jpg}
            \caption{Canny Edge}
        \end{minipage}
        
    \end{figure}
    \begin{figure}[h!]
        \centering
        \begin{minipage}[b]{0.30\linewidth}
            \includegraphics[width=\linewidth]{output/hough_line.jpg}
            \caption{Hough Line Image}
        \end{minipage}
        \hspace{3cm}
        \begin{minipage}[b]{0.30\linewidth}
            \includegraphics[width=\linewidth]{output/intersection_point_output.jpg}
            \caption{Intersection Point Image}
        \end{minipage}
        
    \end{figure}
    
    \begin{figure}[h]
        \centering
        \begin{minipage}[b]{0.30\linewidth}
            \includegraphics[width=\linewidth]{output/grouped.jpg}
            \caption{K-means Clustering Image}
        \end{minipage}
        \hspace{3cm}
        \begin{minipage}[b]{0.30\linewidth}
            \includegraphics[width=\linewidth]{output/output.jpg}
            \caption{Preprocessing Output Image}
        \end{minipage}
        
    \end{figure}
    \newpage
\newpage
    \item Mobile App: The Mobile app for SnapTag has been developed using Flutter.
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.3]{output/home.jpg}
        \caption{Homepage}
        % \label{fig:segmented_image}
    \end{figure}
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.3]{output/add.png}
        \caption{Image Capturing }
        % \label{fig:segmented_image}
    \end{figure}
\end{enumerate}
\newpage
\section{Work Remaining}
\begin{enumerate}
    \item Segmentation: Till now, Line Segmentation has been done which still needs some improvements. After that, our immediate work will be segmentation words and characters. The following is the result produced using traditional Contour-Based Segmentation for lines.
    \item Model Training: The efficiency of the trained model is 82\% and we are working on improving the efficiency.
\end{enumerate}
%\section{Schedule}

%\begin{figure}[ht]
 %   \centering
  %  \includegraphics[scale=0.5]{photo/ganttchart.png}
   % \caption{Gantt Chart}
    %\label{fig:my_label}
%\end{figure}

\newpage

%Reference
\renewcommand\bibname{REFERENCES} % Change heading to References
\bibliographystyle{IEEEtran} % to use IEEE Format for referencing
\addcontentsline{toc}{chapter}{References} % to add references in TOC
\bibliography{library} % specify the .bib file containing reference information 

%Comment this Chapter if you do not need to include Appendix.

%\addcontentsline{toc}{chapter}{Appendix}

\end{figure}

\end{document}
